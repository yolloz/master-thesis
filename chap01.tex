\chapter{Manta Flow platform}

Before we get into the details of data lineage analysis service for embedded code, let us first introduce and describe Manta Flow platform. It is the platform that the service is integrated with and influences many of the design and implementation decisions. It also provides a couple of solution examples for problems we might face that we will use for inspiration.

%---------------------
% SECTION
%---------------------
\section{Data lineage}

Data lineage refers to the tracking of data from its source to its destination and all the transformations and processes it undergoes along the way. It is the complete end-to-end history of data flow, including its origins, where it has been, and how it has been altered.
\par
The purpose of data lineage is to ensure that data is accurate, trustworthy, and complies with regulatory and compliance requirements. It provides transparency into data transformations, allowing organizations to understand how data has been changed. Data lineage is also used to identify and resolve data quality issues and to support data governance and management initiatives. From an engineering point of view, it can help developers to integrate data from multiple sources more efficiently and ensure that data is accurately transferred and transformed throughout the system. Overall, data lineage plays a crucial role in improving the reliability, efficiency, and effectiveness of data-driven decision-making.
\par
Data lineage can be obtained by hand from teams of analysts that map data environments or, more recently, using one of the automated systems that are being developed. Manual data lineage analysis is time- and labor-intensive, so developing automated solutions can provide more up-to-date results and decrease costs.

%---------------------
% SECTION
%---------------------
\section{Manta Flow overview}

Manta Flow is an automated data lineage platform that scans data environments to build and visualize a graph of all data flows within it. This graph enables its users to get better visibility and control of their data processes.
\par
Manta data lineage graph is a graph consisting of nodes (vertices) and edges between them. A node may represent a data source or a transformation, e.g., a database table column, a report dimension or a union of columns. Edges are directed and represent data flow from one node to the other. Each node is identified by a unique path.
\par
Each data environment consists of a different set of databases, tools and other technologies. In order to build a data lineage graph for the entire environment, Manta Flow uses a wide range of proprietary scanners. Each scanner can analyze data lineage for a single data technology (we will use the term \textit{data technology} to uniformly describe databases, ETL/reporting tools and programming languages supported by Manta Flow platform). These partial graphs are then combined in metadata repository to provide the output for the entire environment.

%---------------------
% SECTION
%---------------------
\section{Scanners}

A scanner provides data lineage for a particular data technology. This process consists of multiple steps and varies for each technology based on its type. The data lineage is created from metadata and scripts, so the execution of a scanner is split into two general scenarios: metadata extraction and dataflow analysis.

\subsection{Metadata extraction}

Metadata extraction usually, as the name suggests, extracts metadata required for the dataflow analysis. Other artifacts could be extracted, too, such as scripts of any kind, configuration etc. The purpose of this scenario is to gather all resources needed for dataflow analysis and store them locally so that the analysis can be executed in \textit{offline} mode, that is, without requiring an active connection to any other system.
\par
Database scanners usually go one step further and create so called \textit{data dictionary} from the extracted metadata. Data dictionary stores schema of extracted database resources in a universal data structure that is able to capture different hierarchical structure.
\par
There are multiple reasons why metadata extraction is a standalone scenario. Here is a list of some that are relevant in the context of this thesis:
\begin{itemize}
    \item The analyzed systems are not directly accessible from \textit{MANTA Flow} server. \textit{MANTA Flow Agent} is a component that can be configured to perform the extraction on a remote device which has network access to these systems and then securely transfer the extracted artifacts back to \textit{MANTA Flow} server. 
    \item When errors occur during extraction, the user can fix them before executing dataflow analysis.
    \item In some cases, the user may want or have to provide additional input, e.g., configuration not provided via any available API, additional mapping information etc.
\end{itemize}

\subsection{Dataflow analysis}

Dataflow analysis discovers data flows in target systems using the extracted inputs. The output of such analysis is a data lineage graph. There are multiple steps in this process and the specifics vary between different technologies. Sometimes it in includes translating structured metadata into nodes and edges, sometimes scripts need to be analyzed.


% Description of query service, how it works, similarities to what we are trying to achieve.
%- it has a common interface
%- usually used when the connection details are not entirely known
%- could also be that nothing is known
%- can deduce
%- only resolves queries for which it constructs nodes
%---------------------
% SECTION
%---------------------
\section{Dataflow Query Service}

Dataflow Query Service is a component that helps with processing of embedded SQL queries. It works by selecting the appropriate embedded language scanner for the provided input, executing dataflow analysis and merging the resulting sub-graph into the graph of the original data technology scanner.
\par
Often, data technologies such as reporting or ETL tools use SQL to define data sources. Such SQL queries are used to query data from a connected database without the need to create a dedicated table or a view. Since Manta Flow contains scanners for most common database systems, they are already able to process these queries. Dataflow Query Service groups these scanners in a unified service that can be used by other data technologies.
\par
One of the main benefits of this service is that it uses metadata extracted by each scanner, therefore it has access to extracted database schemas, which allows it to resolve exact columns for queries such as \texttt{SELECT * FROM TABLE\_A}. It is also useful in situations when details about the target systems are unknown or unresolved, the service has the ability to deduce columns from available information and to choose the appropriate scanner from the provided connection details.

\subsubsection{Overview}
In general, there are three pieces of information needed to analyze an SQL query:
\begin{itemize}
    \item Connection data - connection type, connection string, server hostname (at least connection string or server name is needed).
    \item Default data - default database, default schema, connecting user (as applicable) in case connection data is not available or not recognized.
    \item Query or embedded script text.
\end{itemize}
If the data technology scanner has all of the above information, it constructs a \texttt{Connection} object directly, otherwise it is expected to perform connection mapping by retrieving the required data from manual configuration. Connection and query text are inputs for Dataflow Query Service. Next, dictionary mapping is performed, that is, an appropriate target scanner and a persisted data dictionary is selected based on \texttt{Connection} data. The selected scanner is invoked with query text, data dictionary and defaults, which provides the result in form of a \texttt{DataflowQueryResult}. The data technology then uses this result to connect the resulting lineage to the relevant nodes in the host script and to merge the result into the main graph.

\subsubsection{External connections}
One of the main purposes of \texttt{DataflowQueryResult} is to perform connection to external nodes. The purpose of embedded queries is to provide data for further processing, therefore we would like to connect the nodes of the query objects with other nodes in the lineage, e.g., connect database columns with data fields in a report. These connection points are unknown to the query scanner and the host scanner doesn't understand the query in advance, so the result graph contains extra nodes called \textit{pin} nodes.
\par
Pin nodes represent an input to a node representing query parameter or an output from a query resultset column. After the query is analyzed and its graph is created, the service adds these pin nodes and connects them with appropriate edges to the result nodes. Then, the host scanner might provide a mapping from pin nodes to nodes in the host graph. The service creates new edges from pin nodes to host graph nodes (and vice versa, depending on the direction) based on the provided mapping and then contracts the pin nodes, thus effectively connecting the resultset column node or a parameter node with the host graph node. Should any pin node remain unmatched, it is filtered out in filtering task later in the scenario.

\subsubsection{Architecture}
%%% copied from Confluence, can I somehow quote this?
\begin{figure}[ht]\centering
\includegraphics[width=1.0\textwidth]{img/cls.png}
\caption{A simplified class diagram of Dataflow Query Service}
\label{fig01:QS}
\end{figure}  


\texttt{DataflowQueryService} is a common interface that provides methods for client scanners for analyzing SQL queries or creating hierarchical database structures. This interface is implemented by:
\begin{itemize}
    \item Individual database query services that contain logic for providing data flows or hierarchical database structures.
    \item Proxy class called \texttt{DataflowQueryServiceImpl} that wraps all other individual query services. It is a classic implementation of a proxy class that chooses a specific query service based on information provided in the connection from the caller and delegates the operation.    
\end{itemize}

A simplified class diagram is depicted in figure~\ref{fig01:QS}.

% Description of language scanners, their composition, how they work
%- no DDL/dictionaries, only scripts
%- extraction is just a preparation of the input
%- analysis uses worklist algorithm, intermediate dataflow generator, common output
%- differences with database scanners
%- 
%---------------------
% SECTION
%---------------------
\section{Programming Language Scanners}

There are currently three programming language scanners: Bytecode scanner, C\# scanner and Python scanner. Their purpose is to analyze source code and discover data flows there. It proves important to also support this type of data lineage as sometimes the transformation from one data source to the other is implemented in a script or an application. Without this piece of information it is not possible to deliver a complete data lineage.
\par
Programming language scanners are one of the most complex scanners in Manta Flow. The scope of processes that can be expressed in a programming language is much greater and more complex compared to other data technologies. These scanners perform static analysis of the code. Execution of the code is not possible nor wanted and modification of the source code for the purposes of data lineage analysis is usually not possible either because there may be hundreds to thousands of code files included in the analysis which would be very labour intensive. Therefore, static analysis is the only feasible option.
\par
The goal of this static analysis is to find where data is read into the application, track it across all transformations and then find where it is written out. This proved to be a difficult task. There are several approaches for static code analysis, but none of them is aimed at data lineage, so a custom algorithm had to be developed. It uses symbolic analysis and an iterative approach along with multiple optimisations to produce limited results in a limited environment. These limitations are computing power, memory space and time. Manta Flow is expected to run in a common enterprise environment on a machine with a standard multi-core CPU, a reasonable RAM size (e.g. 32GB). Furthermore, the analysis is expected to end in the span of hours up to a few days. Due to that the output focuses on visualizing data reads and writes and flows between them and does not show internal transformations.

\subsection{Dataflow analysis of source code}

Let us describe how this analysis works in more detail. It will help us understand problems and solutions later in this work. All programming language scanners follow a similar workflow, but they implement each step in a little different way due to differences between the languages.

\subsubsection{Source code extraction}

The first step is source code extraction. Before the analysis can begin, all inputs need to be collected and prepared in the format they are expected to be in. The applications usually consist of the application code and of external libraries. When the files are collected, an extraction configuration is generated which captures the structure of the input allows the user to specify which functions or modules should be considered as analysis entry point.
\par
Analysis entry point is the routine that shall be considered as a starting point of a program execution. Usually, it can be the \texttt{main} method in Java or C\# or the module containing \texttt{\_\_main\_\_} block in Python, but in general it can also be any other function, method or module. This defines the starting point of the analysis and from that point function invocations and variable assignments are tracked.
\par
Source code extraction is a standalone step and represents metadata extraction for programming language scanners. All following steps are a part of dataflow analysis.

\subsubsection{Input processing}

In this initial step of dataflow analysis, the extracted inputs are read from file system and pre-processed. In Python, this involves parsing of the source code into an internal representation. There are a couple data structures that need to be created in this step which are used in the following ones.
\par
One of these structures is class hierarchy, which helps with the detection of invocation targets. The other, more important for the algorithm is \textit{call graph}. It captures caller-callee relationship between executables (functions, methods, modules) in the application. A caller is the executable that invokes another executable in its body, a callee is the executable that is being invoked by another executable. Call graph helps to find which other executables need to be analyzed again after the analysis of one, because its result might influence the callers and callees. This will be explained in more detail later.

\subsubsection{Alias analysis}

Aliases are different expressions that might reference the same value. It is important to analyze assignments in the application to resolve these aliases, because a value assigned into one of these expressions has to be propagated into all aliases. Similarly, if a value in an expression is modified, so it is in all of the aliases.

\subsubsection{Symbolic analysis}

This is the core of the analysis. 

\subsubsection{Output transformation}

\subsubsection{Generating data lineage}

