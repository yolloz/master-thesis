\chapter{AWS Glue Scanner}

Analysis, design and implementation of AWS Glue Scanner.

%%----- SECTION -----%%
\section{AWS Glue analysis}

TODO: Introduce that we picked AWS Glue.

\subsection{Overview}

AWS Glue is a serverless data integration service. It is a cloud-based service for creating, running, and monitoring ETL workflows. It requires no setup or management of a server, each execution is managed by Glue and runs as a separate execution unit. It consists of two main parts: \textit{Data Catalog} and \textit{ETL}. Data Catalog helps organizing various data sources in a common metadata catalog, ETL tool then allows creating, and running various ETL jobs and workflows.

\subsubsection{ETL}
The ETL part of AWS Glue runs ETL jobs on Apache Spark. It can run scripts written in Scala or Python. Glue provides a web interface for visual creation of ETL workflows, but allows authoring of generated scripts or providing own scripts. However, self-authored scripts usually can't be modified in the visual interface (unless correct comment annotations are added). 
\par
An ETL job is basically the script that is executed without the need for a server. It supports adding additional parameters, so the scripts can be parameterized and therefore reused. Jobs can be scheduled to run on a schedule or triggered by an event.

\subsubsection{Data Catalog}
Data catalog is a collection of metadata databases which contain tables consisting of data columns. Populating of the data catalog can be either done manually or using automated crawlers and classifiers. A crawler is a process that can crawl several data stores in a single run, enumerating its elements and running classifiers (built-in or custom) on these elements to extract their schema metadata. This metadata is used to create or update tables in the data catalog. Data catalog tables can then be used as data sources and sinks in ETL jobs.


%---------------------- DUMP ----------------------%
\subsection{AWS Glue Metadata Model}

AWS Glue could, on the top-level, be divided into two components: data catalog and ETL service. Data catalog manages data sources and extraction of metadata about the data stored in them. ETL service allows the users to create and manage jobs that load and transform data using data sources from the data catalog. 

Metadata extracted by MANTA Flow AWS Glue scanner could also be separated in two categories based on these components. Data catalog metadata contain information about the structure of data sources and their crawlers/classifiers. This information is not vital for the data lineage analysis but could provide more detail in the output graph. ETL service metadata consists mostly of ETL jobs to be performed on the data, which is the main source for data lineage. These jobs represent Python/Scala scripts for Apache Spark that could be generated by AWS Glue and/or self-authored by users.

\subsubsection{ETL service metadata}
Since the ETL service metadata are more essential for data lineage, we will introduce them first. The important part is a job. A job contains a lot of human-readable metadata about the job, e.g., name, description or timestamps, then some configuration of computational resources and the essential command details. Command details include path to the script stored in AWS S3 storage and its details (language, version) and arguments for the script which allow a dynamic nature of the scripts. Details about this metadata can be found here: Jobs - AWS Glue.

Since the scripts are stored in AWS S3 where we would need to have access, which the user might not want to provide, the script analysis should also be possible by directly providing the script by the user. However, for the best user experience it should also be possible that the scanner directly downloads the script and places it on the convenient place from where the dedicated programming language scanner can read it.

\subsubsection{Important metadata}
Here is a list of properties that need to be extracted from each entity.

\paragraph{Jobs}
\begin{itemize}
    \item Name - identification for the job
    \item Command - an entity describing the command that is run
    \item DefaultArguments - list of default arguments affecting the execution of the command
    \item NonOverridableArguments - list of arguments that cannot be overridden
    \item Connections - a list of connections that this job uses
    \item GlueVersion - a specification of Glue version that defines what library versions are used
    \item Command Name - identification of the job
    \item ScriptLocation - specifies the Amazon S3 path to a script that runs a job
    \item PythonVersion - version of the Python language used in the script (2 or 3)
\end{itemize}

\subsection{Data catalog metadata}

AWS Glue Data Catalog is a centralized metadata repository. It provides a unified interface to store and query information about data formats, schemas, and sources in AWS. Catalog consists of databases, which contain tables with column schemas. Each AWS account has a single Data Catalog in an AWS Region with the 12-digit account ID as the catalog ID. An identification of a catalog would then look like region:account-id.

A catalog contains databases, which represent a logical grouping of tables. Since the catalog is a metadata repository, it directly does not contain any data, it just contains metadata about data sources. A database may then contain any arbitrary grouping of tables.

Tables represent a collection of related data organized in columns and rows. Each table maps to a data source for which it provides connection details, so it is possible to trace real data location. Such data source may be a relational database table, a structured file or any other data source for which there is a connector available. The benefit of the catalog table is that no connection details have to be provided when such table is used in ETL and it also contains schema information for data sources that do not directly provide it (e.g. files).

Crawlers and classifiers are used to automatically verify and add data source schemas to the catalog. They can either be default ones provided by AWS Glue or they can be user-defined. 

For the purposes of data lineage it is important to extract information about databases, tables and their columns as often in the script there is a reference to a data source/target using data catalog identification. Extracting information about crawlers and classifiers is not essential as the result of their job is data catalog. However, an analysis of crawlers and classifiers could be added separately later to help users with their development.

Each AWS account cannot have multiple catalogs (multiple data catalogs belonging to a single connection), the ID of catalog is the ID of the owning AWS account. However, a table may map to a data source not located in the same account/region and these require extra cross-account configuration. Furthermore, some services also allow connection to catalogs of different accounts, provided the cross-account configuration is set up. Unless explicitly specified, the services connect to the default data catalog (belonging to the same account as the current user).

\subsubsection{Important metadata}
Here is a list of properties that need to be extracted from each entity.

\paragraph{Database}
\begin{itemize}
    \item Name - the name of the database
    \item LocationUri - the location of the database (for example, an HDFS path)
    \item Parameters (optional) - key-value pairs defining parameters and properties of the database
    \item TargetDatabase - a DatabaseIdentifier structure that describes a target database for resource linking
    \item CatalogId - the ID of the Data Catalog in which the database resides
\end{itemize}

\paragraph{Table}
\begin{itemize}
    \item Name - the table name
    \item DatabaseName  - the name of the database where the table metadata resides
    \item PartitionKeys - a list of columns by which the table is partitioned
    \item ViewOriginalText - if the table is a view, the original text of the view
    \item ViewExpandedText - if the table is a view, the expanded text of the view
    \item TableType - the type of this table (EXTERNAL\_TABLE, VIRTUAL\_VIEW, etc.)
    \item Parameters (optional) - key-value pairs defining properties associated with the table
    \item TargetTable - a TableIdentifier structure that describes a target table for resource linking
    \item CatalogId - the ID of the Data Catalog in which the table resides
\end{itemize}

\paragraph{Column}
\begin{itemize}
    \item Name - the name of the Column
    \item Type - the data type of the Column
    \item Parameters (optional) - key-value pairs defining properties associated with the column.
\end{itemize}

\subsubsection{Storing metadata}
We need to store the extracted metadata on disk to prepare them for dataflow analysis. They can be partitioned per AWS Glue connection, as each of the connections has different jobs and catalog. Within a connection, we have a common Data Catalog and a list of jobs. We can store the catalog in an arbitrary data structure serializable on disk or we can simply store it in a JSON file (the database seems to have a simple structure, a collection of databases, each contains a collection of tables consisting of columns). Next to the catalog we can have a collection of jobs, from which the main part is the job script (Python or Scala). These scripts can be extracted from AWS S3 but it should be possible to add them manually in case the customer does not want to provide access to their S3 storage. Optionally, a standalone catalog extractor could be available too if the Manta instance was running on a machine with no internet access.

\subsubsection{Column-level lineage}
AWS Glue provides a column-level data lineage. The base entities in transformation scripts are data frames, either Spark’s DataFrame or AWS Glue’s DynamicFrame. These frames have a table-like structure and all transformations require an input and an output data frame. We don’t have the exact information about the data in these data frames, but we know what the source columns are and how the transformations behave, so we can determine column-level lineage.

A script in AWS Glue is expected to contain AWS Glue and PySpark code, optionally with some additional helper libraries (most likely to connect to data sources outside of data catalog), but the tool is intended to run Spark transformations.

\subsection{AWS Glue API}
It is common to manage AWS settings through AWS console, which is a web user interface. AWS also provides a wide variety of SDKs for many popular programming languages that allow interacting with AWS services. One of the supported languages is Java, so it is natural we use this SDK. 

AWS java SDK is currently in version 2.x and is available on Maven Maven Repository: software.amazon.awssdk under Apache 2.0 license. This SDK contains multiple sub-packages for each AWS service including AWS Glue. There is a library of examples available on GitHub. The API has a nice and rich documentation AWS Glue API - AWS Glue. This API should suffice to extract all necessary information from Glue, mainly ETL job script locations (usually an S3 URI) and reading data catalog (not vital for PoC but very useful as ETL jobs can reference data catalog tables).

To be able to access the services it is important to have programmatic credentials (this is probably what users will need to set up themselves). There are multiple ways on how to provide these credentials. The default way is to store them in a file on a predefined location or save them in an environment variable. Then, SDK will use these credentials for each API call, thus removing the need to use and load credentials in code. Alternatively, it is possible to provide a custom credentials provider that provides these credentials in a different way (static access, custom environment variable etc.). The best option is to store the credentials in the Connection settings and have them injected by Spring. However, allowing users to provide them through the configuration file or environment variable should be possible as an option too. For more information see Provide temporary credentials to the SDK - AWS SDK for Java 2.x. 

When accessing AWS Glue, it is necessary to specify AWS Region, which should also be a part of Connection configuration and can be provided in a similar way as access credentials.

\subsection{AWS Glue Connector design}
AWS Glue Connector takes care of extracting metadata from AWS Glue, storing them and resolving the inputs for dataflow analysis.

The connector can be divided into 4 common main components:
\begin{enumerate}
    \item Extractor
    \item Model
    \item Resolver
    \item Reader
\end{enumerate}


\subsubsection{Extractor}
The extractor takes care of connecting to AWS Glue connection and extracting the required metadata, saving them to the file system. To extract the metadata, AWS Java v2 SDK is used. The SDK returns data in its own Java objects. The data that we need to extract are data catalog database schemas and code scripts (Python or Scala). While the scripts can remain in their file format, we need to somehow store the data catalog metadata.

These are well structured: a database contains tables which consist of columns. Such structure could easily be captured in JSON, which is both human- and machine-readable format. It should be friendly for both as sometimes the catalog cannot be extracted (e.g., customer can’t provide external access). In such situations it should be possible to provide the schema manually. In MANTA, it is common to use the schema that the API provides natively in exports, because it can be easily provided by customers if they need to. With AWS Glue, there is no direct export, but it can be obtained using aws-cli.

aws-cli supports multiple output formats including JSON. On the other hand, Java SDK uses object mapping. Luckily, the objects are constructed from a JSON in a response body which is in the same format as aws-cli output and can be accessed with just a little effort. Therefore, the best approach to store the data catalog extract output would be using these JSON formats.

After the JSON metadata of data catalog is extracted, we should create a specific Data Dictionary (manta-connector-common-dictionary). This allows us to use query service to resolve creating dynamic\_frame data sources from data catalog and thus reuse some code. With that, language scanner can construct such nodes directly without the need to know anything about embedded code service. The schema of data catalog is quite simple, so creating  the dictionary would be rather straight-forward. The open problem is mapping data catalog data source (there are only metadata in data catalog) to the real source of the data - a file on a specific location or database table. It could either be done using some concepts within data dictionary/query service or manually mapped by Glue scanner at the end of dataflow generation. 

The extraction process requires a Glue connection. Such connection consists of AWS region definition and access credentials. A connection defines the range of a single extraction run. Within that, a data catalog and a list of jobs is extracted. All jobs share the same data catalog.

As we mentioned above, data catalog extraction consists of extracting the lists of databases and tables in JSON format. For jobs, we need the job code file and its configuration (also in JSON). This configuration contains information about the programming language (Python or Scala), its version and parameters that the job can be launched with. 

A job can have default arguments and non-overridable arguments. Default arguments are used when no other arguments are specified. Each job run can specify own arguments, we should implement a way for users to provide these arguments (either directly as a file in the input directory, value mapping etc.) as they have significant consequences - jobs can use these arguments in the scripts similarly to standard program arguments. Moreover, additional python libraries and extra .py files are provided using job arguments - AWS Glue reads these arguments and prepares the runtime environment accordingly. 

Additional libraries and .py files are provided in --additional-python-modules and --extra-py-files arguments (Using Python libraries with AWS Glue - AWS Glue). These values can be an S3 path to a single-file library, a zip file containing the library, or the name (and version, optionally) of a library that should be installed from pip. Apart from pip libraries, we also need to extract and save these additional resources so they can be provided for Python scanner.

Glue extractor workflow
Extractor workflow
The proposed file structure is then as follows:

A folder for each connection

within the folder on top level are JSONs describing the data catalog

within the folder on top level is data dictionary for the data catalog

within the folder are sub-folders each containing a single job

within a job directory there is

a code file

a job configuration JSON

a folder containing additional libraries - those that reference a file or a zip archive

a folder containing extra py files


Proposed file structure
\subsubsection{Model}
Model provides a common data interface for resolver. 

\subsubsection{Resolver}
Resolver reads the stored input files (code scripts and data catalog JSONs) and provides their Java object representation to reader. The output should be a single object that contains both data catalog metadata and transformation scripts.

\subsubsection{Reader}
Reader takes care of running the scenarios. It retrieves an AWS Glue instance representation from resolver, performs necessary filtering on jobs to be analyzed and then passes the filtered instance to data flow analyzer.

Connector flow chart

 

 