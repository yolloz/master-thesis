\chapter{Requirements and Analysis}

In this chapter we follow up on the Manta Flow platform description and explain the problem of analysing embedded code in this context. We analyze multiple technologies that support embedded code and formulate our requirements for the solution. Then we analyze how to integrate embedded code analysis into scanners.

%%----- SECTION -----%% An extended problem overview, analysis of supported/desired technologies that support embedded code.
\section{Problem overview}
% Why we analyze scripts
Analysis of scripts is an integral part of many scanners supported in Manta Flow. The scripts improve the usability of the data technology as they enable user-defined ways to modify and work with data and even might be the only way to do so. Stored procedures and functions in databases enable data manipulation on top of data storage which can be especially beneficial if the database is accessed from an environment with limited database support. ETL platform scripts allow users to define their own data transformations on top of the well-defined ones. In reporting tools, the scripts provide a handy way to pre-process and define source data for data visualizations.
\par % some scripts are easy to analyze
In most of the cases, these scripts use a more-or-less standardized language native to the platform. Analyzing native scripts has been a standard part of scanners. It provides a lot of value as it covers a great part of data lineage. Since such language is usually an internal part of a particular data technology, its processing algorithm does not need to be shared with other components and it can utilize the internal representation of other entities unique to that data technology. Additionally, these languages do not usually have a strong expressive power, meaning that the breadth of ideas that can be represented in them is limited to the scope of the data technology and is usually  lower than that of standard programming languages.
\par % sometimes a more complex language is used for scripts
Analyzing code written in standard programming languages is considerably more difficult, mainly because the expressive power is that much stronger. It often contain expressions or entire algorithms not relevant for the data lineage, however that only becomes apparent after the analysis. That makes programming language scanners the most complex ones. However, this ability is useful for embedded code scripts. Certain data technologies provide an environment where these scripts can be executed, thus utilizing the potential of strong, well-defined and well-known languages.

\subsection{Analyzing embedded code}
% why we want to analyze embedded code scripts
Adding support for embedded code scripts in a data technology might initially sound like a great idea, but like everything else, it comes with its positives and negatives. We have already mentioned most of the positives. The main negatives from software development point of view are limited debugging, testability and version control support. As already mentioned, embedded code is executed in a custom internal runtime environment of the data technology and its replica for development is not always available. For that reason there is an increased demand for systems that can bridge this gap and verify the embedded code correctness. One of the ways to check it is utilising data lineage analysis to verify that the data flow as expected.
\par % what we need to analyze embedded code
Supporting the analysis of embedded code in Manta Flow is not as simple as running the particular scanner with the code as its input. Since the analysis of programming languages is so complex, we want to use the existing scanners as much as possible. However, original implementations of these scanners were developed to be able to process applications written in the particular programming language. They are designed to run in standalone scenarios and their configuration mostly defines where the application can be found. We need to modify these scanners so they can be reused by other independent data technology scanners in their scenarios in a meaningful way.
\par % how query service does it
A similar problem has been solved by Dataflow Query Service which allows other scanners to process database queries. These database queries need to be processed by a particular database scanner, because different databases use different SQL dialects and also because Query Service results are enriched with the mapping to the actual data sources that has been analyzed by that scanner, if possible. We will inspire by its design because it solves two similar problems: providing a partial lineage graph to be used in another lineage graph and executing the analysis of one scanner in the scope of another.
\par % how we shall do it
Our solution shall provide an embedded code service which can be integrated and used in other scanner scenarios. It shall provide an interface to configure the input and execute the analysis of the provided embedded code and it shall provide an interface to integrate the resulting lineage graph with the lineage graph of another data technology.

%%----- SECTION -----%%
\section{Source technology analysis}

Before we dive further into the analysis, we need to examine which source technologies, whether currently supported by Manta Flow or planned, support embedded code. This overview will give us a better understanding of the range of embedded code use-cases as well as help us in choosing a candidate for a proof-of-concept implementation.

\subsubsection{Hive}
Hive is a distributed data warehouse system that allows users to read, write and manage big volumes of data using SQL. Starting from version 0.13.0, it supports writing user-defined functions in Java which accept parameters and return a value. The function is implemented by a class that extends a Hive UDF base class. These base classes define methods that shall be implemented by the function and will be invoked in specific order on SQL query execution. This class is then supposed to be packaged in a JAR referenced by a URI from which it will be loaded into the environment~\cite{hive}.

\subsubsection{Microsoft SQL Server}
Microsoft SQL Server enables users to implement stored procedures, triggers, user-defined types, user-defined functions (scalar and table valued), and user-defined aggregate functions using any .NET Framework language, including Microsoft Visual Basic .NET and Microsoft Visual C\#. They can be implemented by arbitrary classes and methods as long as they are properly annotated. These annotations facilitate the lookup and binding between SQL Server and embedded code. Compiled code is distributed in DLL and loaded into environment using SQL syntax~\cite{mssql}.

\subsubsection{SQL Server Integration Services (SSIS)}
SSIS is a platform for data integration and transformation solutions. It provides graphical tools for building ETL workflows, but it is also possible to create custom objects programmatically in C\# or Visual Basic. These include tasks, connection managers, log providers, enumerators and data flow components. Implementations of custom objects are expected to extend one of the base classes provided by SSIS, to override required methods and to use proper attributes. These objects are then distributed as a compiled class library. This is a similar approach as that of MSSQL~\cite{ssis}.

\subsubsection{PostgreSQL}
By default, PostgreSQL supports functions written in C, but theoretically users may use any language, as long as it can be made compatible with C, e.g. C++. However, that is often difficult due to different calling conventions, so its safe to assume C language is used. The function definitions are supposed to use macros from \texttt{postgres.h} header file, but otherwise are common C functions. The code is compiled and dynamically loaded into the environment using SQL. There is currently no plan to support analyzing C language, so this data technology is mentioned only for completeness~\cite{postgresql}.

\subsubsection{Snowflake}
Snowflake Data Cloud is a cloud-based data storage and analytics service. It is common to use SQL to interact with data in Snowflake and embedded code is integrated in a similar way in the form of functions and stored procedures. Apart from SQL, these can be written in multiple programming languages - Java, Scala, JavaScript or Python. Each language used has (slightly) different capabilities and requirements. In case of JavaScript, it can be used to execute SQL statements and interact with the result to provide a return value. Java, Scala and Python scripts have to contain a function or a method with the first argument of type \texttt{Session} from Snowflake's Snowpark library. This argument will be populated by Snowflake when the procedure/function is invoked and is used for interaction with Snowflake platform~\cite{snowflake}.

\subsubsection{Databricks}
Databricks is a web-based data platform that combines data warehouses and data lakes with analytics build on Apache Spark and IPython-style notebooks. These notebooks are interactive computational environments. They consist of a sequential combination of cells which may contain rich text, embedded code, data visualization etc. Embedded code cells may be written in Python, Scala, SQL or R and it is possible to combine cells written in different languages in one notebook. During a notebook execution, cells written in the same language are executed in the same environment and may interact with other language environments using shared context of Apache Spark~\cite{databricks}.

\subsubsection{AWS Glue}
AWS Glue is an ETL tool that supports interactive ETL pipeline creation and then generating code for Apache Spark in Python or Scala that executes the pipeline. This code can then be further modified. Compared to other data technologies, AWS Glue is built entirely on embedded code. That means that each ETL job is executed entirely by a single Python/Scala script as opposed to only parts of data operations in other data technologies~\cite{awsglueintro}. 

\subsubsection{Other data technologies}
Besides the data technologies already described, there are others that provide embedded code integration and are supported in Manta Flow. These follow similar principles as some that were already described before, so we will not cover them in detail. However, we list them below for completeness:
\begin{itemize}
    \item Talend supports extending the functionalities of a Talend Job using custom Java commands~\cite{talend}.
    \item Google BigQuery supports defining functions written in JavaScript~\cite{bigquery}.
    \item StreamSets allows creating custom StreamSets processors in Java~\cite{streamsets}.
    \item Informatica supports creating custom components with Java~\cite{informatica}.
    \item Azure Data Factory supports creating Custom activity with own data movement or transformation logic in C\# that can be added to a pipeline~\cite{adf}.
    \item SAS supports running Python statements within a SAS session~\cite{sas}. Additionally, there are multiple Python packages for interacting with SAS from Python.
\end{itemize}

\subsection{Embedded code usage philosophy}
Based on the description of embedded code usages, we can observe a few repeating patterns. These will help us design embedded code service.
\par
We can see that database systems use embedded code in the form of user-defined functions or stored procedures. They can then be invoked as a part of an SQL query or an SQL script. They often return a value and may receive arguments.
\par
Another common use-case is to define a custom transformation or a task in an ETL workflow. The details of this use-case vary more than those in database systems but in general they implement a specific interface which functions are invoked in a pre-determined order by the data technology.
\par
Next observation relies to the programming language being used. Most often we can see Java or Scala, .NET languages (mainly C\# or Visual Basic) and Python. Sometimes also JavaScript is used and there is one case of R and C, but we shall ignore them as there isn't currently a language scanner implemented in Manta Flow for them.
\par
In compiled static-typed languages (Java and C\#), it is common to tag the classes that shall be used as embedded code and require a rigid interface, either by extending a base class or using annotations. The code is distributed in compiled form and loaded dynamically. Interaction with the data technology is facilitated through an object that is provided as a method argument or a property of a base class.
\par
As Python is interpreted and not compiled, it is possible to inject the embedded code into a different code to create a new script. That allows use-cases where some code is executed before embedded code which defines some variables, functions, classes etc. The embedded code may then directly read these identifiers without having to declare them, which is used to provide interface for interacting with data technology. To successfully analyze such approach, it is important to understand and simulate these assignments.

%%----- SECTION -----%% Functional and qualitative requirements
\section{Requirements}

There are a couple of requirements that the embedded code service needs to fulfill. 

\subsection{Functional requirements}
\begin{enumerate}
    \item Provided a code script (string/file, not important implementation detail) and configuration, the service analyzes the script and delivers lineage graph for that script
    \item The service can merge the lineage graph with the lineage graph of parent technology when provided a node to be merged with    
    \item The service can support multiple source technologies - technologies that support usage of user-defined embedded code
\end{enumerate}

\subsection{Qualitative (non-functional) requirements}
\begin{enumerate}
    \item The service should be optimized to handle tens to hundreds of scripts for a specific combination of technology-scanner in one analysis - the limitation should be the speed of the scanner, not speed of the service
    \item Extending supported technologies should be simple - adding a new technology should only include collecting its configuration in the technology scanner and implementing this configuration in embedded code service
    \item Maximizing code reuse - reuse the existing scanners and logic
    \item Minimizing code duplication - no logic should be written on more than one place
\end{enumerate}